{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d60c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم استيراد المكتبات بنجاح!\n"
     ]
    }
   ],
   "source": [
    "# الخلية 1: استيراد المكتبات الأساسية\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"تم استيراد المكتبات بنجاح!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48168fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم تحميل مجموعتي بيانات التدريب والاختبار بنجاح من المسار المحدد.\n"
     ]
    }
   ],
   "source": [
    "# الخلية 2: تحميل مجموعات البيانات من المسار المحدد\n",
    "\n",
    "# المسار الكامل للمجلد\n",
    "data_path = r'C:\\Users\\fasy\\Desktop\\kaggle2'\n",
    "\n",
    "# أسماء الملفات\n",
    "train_file_name = 'train (2).csv'\n",
    "test_file_name = 'test (2).csv'\n",
    "\n",
    "try:\n",
    "    # بناء المسار الكامل لكل ملف وتحميله\n",
    "    train_df = pd.read_csv(f'{data_path}\\\\{train_file_name}')\n",
    "    test_df = pd.read_csv(f'{data_path}\\\\{test_file_name}')\n",
    "    \n",
    "    print(\"تم تحميل مجموعتي بيانات التدريب والاختبار بنجاح من المسار المحدد.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"خطأ: لم يتم العثور على الملفات في المسار التالي: {data_path}\")\n",
    "    print(\"الرجاء التأكد من صحة المسار وأسماء الملفات.\")\n",
    "except Exception as e:\n",
    "    print(f\"حدث خطأ غير متوقع أثناء التحميل: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec7a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- أول 5 صفوف من بيانات التدريب (train_df): ---\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "--- معلومات عامة عن بيانات التدريب (train_df.info()): ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "\n",
      "شكل بيانات التدريب (عدد الصفوف، عدد الأعمدة): (7613, 5)\n"
     ]
    }
   ],
   "source": [
    "# الخلية 3: نظرة أولية على بيانات التدريب\n",
    "\n",
    "print(\"--- أول 5 صفوف من بيانات التدريب (train_df): ---\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- معلومات عامة عن بيانات التدريب (train_df.info()): ---\")\n",
    "# .info() تعطينا عدد القيم غير الفارغة (Non-Null Count) لكل عمود وأنواع البيانات (Dtype)\n",
    "train_df.info()\n",
    "\n",
    "print(f\"\\nشكل بيانات التدريب (عدد الصفوف، عدد الأعمدة): {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de00b848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- أول 5 صفوف من بيانات الاختبار (test_df): ---\n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
      "\n",
      "--- معلومات عامة عن بيانات الاختبار (test_df.info()): ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n",
      "\n",
      "شكل بيانات الاختبار (عدد الصفوف، عدد الأعمدة): (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# الخلية 4: نظرة أولية على بيانات الاختبار\n",
    "\n",
    "print(\"--- أول 5 صفوف من بيانات الاختبار (test_df): ---\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\n--- معلومات عامة عن بيانات الاختبار (test_df.info()): ---\")\n",
    "test_df.info()\n",
    "\n",
    "print(f\"\\nشكل بيانات الاختبار (عدد الصفوف، عدد الأعمدة): {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2c0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- القيم المفقودة في بيانات التدريب (train_df.isnull().sum()): ---\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      "\n",
      "--- نسبة القيم المفقودة في بيانات التدريب: ---\n",
      "id           0.000000\n",
      "keyword      0.801261\n",
      "location    33.272035\n",
      "text         0.000000\n",
      "target       0.000000\n",
      "dtype: float64\n",
      "--------------------------------------------------\n",
      "\n",
      "--- القيم المفقودة في بيانات الاختبار (test_df.isnull().sum()): ---\n",
      "id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n",
      "\n",
      "--- نسبة القيم المفقودة في بيانات الاختبار: ---\n",
      "id           0.000000\n",
      "keyword      0.796813\n",
      "location    33.864542\n",
      "text         0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# الخلية 5: التحقق من القيم المفقودة في مجموعتي البيانات\n",
    "\n",
    "print(\"--- القيم المفقودة في بيانات التدريب (train_df.isnull().sum()): ---\")\n",
    "print(train_df.isnull().sum())\n",
    "# يمكن أيضًا عرضها كنسبة مئوية لفهم أفضل لحجم النقص\n",
    "print(\"\\n--- نسبة القيم المفقودة في بيانات التدريب: ---\")\n",
    "print(train_df.isnull().sum() / len(train_df) * 100)\n",
    "print(\"-\" * 50) # خط فاصل للتوضيح\n",
    "\n",
    "print(\"\\n--- القيم المفقودة في بيانات الاختبار (test_df.isnull().sum()): ---\")\n",
    "print(test_df.isnull().sum())\n",
    "# يمكن أيضًا عرضها كنسبة مئوية لفهم أفضل لحجم النقص\n",
    "print(\"\\n--- نسبة القيم المفقودة في بيانات الاختبار: ---\")\n",
    "print(test_df.isnull().sum() / len(test_df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201a89f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تمت معالجة القيم المفقودة في عمودي 'keyword' و 'location' بنجاح لكلتا مجموعتي البيانات.\n",
      "\n",
      "--- التحقق من القيم المفقودة بعد المعالجة في train_df: ---\n",
      "id          0\n",
      "keyword     0\n",
      "location    0\n",
      "text        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "--- التحقق من القيم المفقودة بعد المعالجة في test_df: ---\n",
      "id          0\n",
      "keyword     0\n",
      "location    0\n",
      "text        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# الخلية 6: معالجة القيم المفقودة\n",
    "\n",
    "# تعبئة القيم المفقودة في عمود 'keyword'\n",
    "train_df['keyword'].fillna('no_info', inplace=True)\n",
    "test_df['keyword'].fillna('no_info', inplace=True)\n",
    "\n",
    "# تعبئة القيم المفقودة في عمود 'location'\n",
    "train_df['location'].fillna('no_info', inplace=True)\n",
    "test_df['location'].fillna('no_info', inplace=True)\n",
    "\n",
    "print(\"تمت معالجة القيم المفقودة في عمودي 'keyword' و 'location' بنجاح لكلتا مجموعتي البيانات.\")\n",
    "\n",
    "# التحقق مرة أخرى للتأكد من عدم وجود قيم مفقودة بعد المعالجة\n",
    "print(\"\\n--- التحقق من القيم المفقودة بعد المعالجة في train_df: ---\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\n--- التحقق من القيم المفقودة بعد المعالجة في test_df: ---\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dcf04dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بدء تنزيل موارد NLTK الضرورية: 'wordnet' و 'omw-1.4'...\n",
      "تم تنزيل موارد NLTK المطلوبة بنجاح.\n",
      "\n",
      "بعد التنزيل، أعد تشغيل الخلية 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fasy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fasy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# الخلية 8: تنزيل موارد NLTK الضرورية فقط (wordnet و omw-1.4)\n",
    "\n",
    "import nltk\n",
    "\n",
    "print(\"بدء تنزيل موارد NLTK الضرورية: 'wordnet' و 'omw-1.4'...\")\n",
    "\n",
    "try:\n",
    "    # تنزيل WordNet، وهو ضروري للlemmatization\n",
    "    nltk.download('wordnet')\n",
    "    # Open Multilingual Wordnet، غالبًا ما يكون مطلوبًا لعمل WordNet بشكل سليم\n",
    "    nltk.download('omw-1.4')\n",
    "    \n",
    "    print(\"تم تنزيل موارد NLTK المطلوبة بنجاح.\")\n",
    "except Exception as e:\n",
    "    print(f\"حدث خطأ أثناء تنزيل موارد NLTK: {e}\")\n",
    "    print(\"يرجى التأكد من اتصالك بالإنترنت.\")\n",
    "\n",
    "print(\"\\nبعد التنزيل، أعد تشغيل الخلية 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf4088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بدء المعالجة المسبقة المحسنة لعمود 'text'...\n",
      "تمت المعالجة المسبقة المحسنة لعمود 'text' بنجاح. تم تحديث عمود 'cleaned_text'.\n",
      "\n",
      "--- مقارنة بين 'text' و 'cleaned_text' بعد التحسين في train_df: ---\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0         deed reason earthquake may allah forgive u  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  resident asked shelter place notified officer ...  \n",
      "3  people receive wildfire evacuation order calif...  \n",
      "4  got sent photo ruby alaska smoke wildfire pour...  \n",
      "\n",
      "--- مقارنة بين 'text' و 'cleaned_text' بعد التحسين في test_df: ---\n",
      "                                                text  \\\n",
      "0                 Just happened a terrible car crash   \n",
      "1  Heard about #earthquake is different cities, s...   \n",
      "2  there is a forest fire at spot pond, geese are...   \n",
      "3           Apocalypse lighting. #Spokane #wildfires   \n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                        happened terrible car crash  \n",
      "1  heard earthquake different city stay safe ever...  \n",
      "2  forest fire spot pond goose fleeing across str...  \n",
      "3               apocalypse lighting spokane wildfire  \n",
      "4                 typhoon soudelor kill china taiwan  \n"
     ]
    }
   ],
   "source": [
    "# الخلية 13: تعديل دالة المعالجة المسبقة للنصوص وإعادة تطبيقها\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# تأكد من أن موارد NLTK (stopwords, wordnet, omw-1.4) تم تنزيلها\n",
    "# إذا لم تكن متأكدًا، قم بتشغيل الخلية 8 مرة أخرى قبل هذه الخلية\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "    WordNetLemmatizer()\n",
    "except LookupError:\n",
    "    print(\"موارد NLTK غير موجودة. يرجى تشغيل الخلية 8 أولاً لتنزيلها.\")\n",
    "    # يمكننا هنا رفع استثناء لضمان عدم المتابعة بدون الموارد\n",
    "    raise\n",
    "\n",
    "def preprocess_text_v2(text):\n",
    "    \"\"\"\n",
    "    نسخة محسنة من دالة المعالجة المسبقة للنصوص:\n",
    "    1. تحويل النص إلى أحرف صغيرة.\n",
    "    2. إزالة الروابط (URLs).\n",
    "    3. إزالة أسماء المستخدمين (@mentions).\n",
    "    4. معالجة الهاشتاجات (إزالة الرمز # والاحتفاظ بالكلمة).\n",
    "    5. إزالة الرموز التعبيرية.\n",
    "    6. إزالة علامات الترقيم.\n",
    "    7. إزالة الأرقام.\n",
    "    8. إزالة مسافات إضافية.\n",
    "    9. إزالة كلمات التوقف (stop words).\n",
    "    10. استخلاص جذر الكلمة (Lemmatization).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. تحويل النص إلى أحرف صغيرة\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. إزالة الروابط (URLs)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. إزالة أسماء المستخدمين (@mentions)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 4. معالجة الهاشتاجات (إزالة # والاحتفاظ بالكلمة)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text) # \\1 يشير إلى المجموعة الأولى الملتقطة (الكلمة بعد #)\n",
    "    \n",
    "    # 5. إزالة الرموز التعبيرية (تعابير نمطية شائعة للرموز التعبيرية)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # 6. إزالة علامات الترقيم\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 7. إزالة الأرقام\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 8. إزالة مسافات إضافية قد تنتج عن التنظيف\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 9. إزالة كلمات التوقف (stop words) + كلمات مخصصة\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'amp', 'co', 'https', 't'} # أمثلة لكلمات قديمة لا تضيف معنى\n",
    "    all_stop_words = stop_words.union(custom_stopwords)\n",
    "\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [word for word in text_tokens if word not in all_stop_words]\n",
    "    text = \" \".join(filtered_text)\n",
    "    \n",
    "    # 10. استخلاص جذر الكلمة (Lemmatization)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# تطبيق دالة المعالجة المسبقة المحسنة على عمود 'text'\n",
    "print(\"بدء المعالجة المسبقة المحسنة لعمود 'text'...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(preprocess_text_v2)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(preprocess_text_v2)\n",
    "\n",
    "print(\"تمت المعالجة المسبقة المحسنة لعمود 'text' بنجاح. تم تحديث عمود 'cleaned_text'.\")\n",
    "\n",
    "# عرض أول 5 صفوف من عمودي 'text' و 'cleaned_text' لبيانات التدريب للمقارنة\n",
    "print(\"\\n--- مقارنة بين 'text' و 'cleaned_text' بعد التحسين في train_df: ---\")\n",
    "print(train_df[['text', 'cleaned_text']].head())\n",
    "\n",
    "print(\"\\n--- مقارنة بين 'text' و 'cleaned_text' بعد التحسين في test_df: ---\")\n",
    "print(test_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43455b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم إنشاء الميزات الإضافية بنجاح.\n",
      "\n",
      "أول 5 صفوف من train_df مع الميزات الجديدة:\n",
      "   text_len  word_count  punct_count  upper_case_ratio  has_keyword  \\\n",
      "0        69          13            1          0.144928            0   \n",
      "1        38           7            1          0.131579            0   \n",
      "2       133          22            3          0.015038            0   \n",
      "3        65           8            2          0.015385            0   \n",
      "4        88          16            2          0.034091            0   \n",
      "\n",
      "   has_location  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "\n",
      "أول 5 صفوف من test_df مع الميزات الجديدة:\n",
      "   text_len  word_count  punct_count  upper_case_ratio  has_keyword  \\\n",
      "0        34           6            0          0.029412            0   \n",
      "1        64           9            3          0.015625            0   \n",
      "2        96          19            2          0.010417            0   \n",
      "3        40           4            3          0.050000            0   \n",
      "4        45           8            0          0.088889            0   \n",
      "\n",
      "   has_location  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "# الخلية 14: هندسة الميزات الإضافية\n",
    "\n",
    "import string # تأكد من استيرادها هنا أيضاً إذا لم تكن مستوردة في خلية سابقة قبل هذه الخلية\n",
    "import pandas as pd # تأكد من استيرادها هنا أيضاً إذا لم تكن مستوردة\n",
    "\n",
    "# 1. ميزات من عمود 'text' الأصلي\n",
    "# طول التغريدة (عدد الأحرف)\n",
    "train_df['text_len'] = train_df['text'].apply(len)\n",
    "test_df['text_len'] = test_df['text'].apply(len)\n",
    "\n",
    "# عدد الكلمات في التغريدة\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# عدد علامات الترقيم\n",
    "train_df['punct_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test_df['punct_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# نسبة الأحرف الكبيرة (capital letters)\n",
    "train_df['upper_case_ratio'] = train_df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "test_df['upper_case_ratio'] = test_df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# 2. ميزات من عمود 'keyword' و 'location'\n",
    "# هل الكلمة المفتاحية موجودة؟ (حتى لو كانت 'no_info' بعد المعالجة)\n",
    "train_df['has_keyword'] = train_df['keyword'].apply(lambda x: 0 if x == 'no_info' else 1)\n",
    "test_df['has_keyword'] = test_df['keyword'].apply(lambda x: 0 if x == 'no_info' else 1)\n",
    "\n",
    "# هل الموقع موجود؟ (حتى لو كان 'no_info' بعد المعالجة)\n",
    "train_df['has_location'] = train_df['location'].apply(lambda x: 0 if x == 'no_info' else 1)\n",
    "test_df['has_location'] = test_df['location'].apply(lambda x: 0 if x == 'no_info' else 1)\n",
    "\n",
    "\n",
    "print(\"تم إنشاء الميزات الإضافية بنجاح.\")\n",
    "print(\"\\nأول 5 صفوف من train_df مع الميزات الجديدة:\")\n",
    "print(train_df[['text_len', 'word_count', 'punct_count', 'upper_case_ratio', 'has_keyword', 'has_location']].head())\n",
    "\n",
    "print(\"\\nأول 5 صفوف من test_df مع الميزات الجديدة:\")\n",
    "print(test_df[['text_len', 'word_count', 'punct_count', 'upper_case_ratio', 'has_keyword', 'has_location']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c9e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بدء المعالجة المسبقة لعمود 'text'...\n",
      "تمت المعالجة المسبقة لعمود 'text' بنجاح. تم إنشاء عمود 'cleaned_text'.\n",
      "\n",
      "--- مقارنة بين 'text' و 'cleaned_text' في train_df: ---\n",
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                    deed reason may allah forgive u  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  resident asked shelter place notified officer ...  \n",
      "3         people receive evacuation order california  \n",
      "4             got sent photo ruby smoke pours school  \n",
      "\n",
      "--- مقارنة بين 'text' و 'cleaned_text' في test_df: ---\n",
      "                                                text  \\\n",
      "0                 Just happened a terrible car crash   \n",
      "1  Heard about #earthquake is different cities, s...   \n",
      "2  there is a forest fire at spot pond, geese are...   \n",
      "3           Apocalypse lighting. #Spokane #wildfires   \n",
      "4      Typhoon Soudelor kills 28 in China and Taiwan   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                        happened terrible car crash  \n",
      "1            heard different city stay safe everyone  \n",
      "2  forest fire spot pond goose fleeing across str...  \n",
      "3                                apocalypse lighting  \n",
      "4                 typhoon soudelor kill china taiwan  \n"
     ]
    }
   ],
   "source": [
    "# الخلية 7: المعالجة المسبقة لعمود 'text'\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# تحميل موارد NLTK إذا لم تكن موجودة بالفعل\n",
    "# قد تحتاج لتشغيل هذا السطر مرة واحدة فقط في بيئة Jupyter جديدة\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "    WordNetLemmatizer()\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4') # Open Multilingual Wordnet (often needed for wordnet)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    تقوم هذه الدالة بمعالجة مسبقة للنصوص عن طريق:\n",
    "    1. تحويل النص إلى أحرف صغيرة (lowercase).\n",
    "    2. إزالة الروابط (URLs).\n",
    "    3. إزالة الهاشتاجات وأسماء المستخدمين (@mentions).\n",
    "    4. إزالة علامات الترقيم.\n",
    "    5. إزالة الأرقام.\n",
    "    6. إزالة كلمات التوقف (stop words).\n",
    "    7. استخلاص جذر الكلمة (Lemmatization).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. تحويل النص إلى أحرف صغيرة\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. إزالة الروابط (URLs)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 3. إزالة الهاشتاجات وأسماء المستخدمين (@mentions)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # 4. إزالة علامات الترقيم\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 5. إزالة الأرقام\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # إزالة مسافات إضافية قد تنتج عن التنظيف\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6. إزالة كلمات التوقف (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "    text = \" \".join(filtered_text)\n",
    "    \n",
    "    # 7. استخلاص جذر الكلمة (Lemmatization)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# تطبيق دالة المعالجة المسبقة على عمود 'text' في train_df و test_df\n",
    "print(\"بدء المعالجة المسبقة لعمود 'text'...\")\n",
    "train_df['cleaned_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"تمت المعالجة المسبقة لعمود 'text' بنجاح. تم إنشاء عمود 'cleaned_text'.\")\n",
    "\n",
    "# عرض أول 5 صفوف من عمودي 'text' و 'cleaned_text' لبيانات التدريب للمقارنة\n",
    "print(\"\\n--- مقارنة بين 'text' و 'cleaned_text' في train_df: ---\")\n",
    "print(train_df[['text', 'cleaned_text']].head())\n",
    "\n",
    "print(\"\\n--- مقارنة بين 'text' و 'cleaned_text' في test_df: ---\")\n",
    "print(test_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aae4248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم تحويل النصوص إلى ميزات رقمية باستخدام TF-IDF بنجاح.\n",
      "شكل مصفوفة ميزات التدريب (X_train_tfidf): (7613, 3262)\n",
      "شكل مصفوفة ميزات الاختبار (X_test_tfidf): (3263, 3262)\n"
     ]
    }
   ],
   "source": [
    "# الخلية 9: تحويل النصوص إلى ميزات رقمية باستخدام TF-IDF Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# تهيئة TF-IDF Vectorizer\n",
    "# max_features تحدد عدد الكلمات (الميزات) الأكثر تكراراً وأهمية لتمثيل النصوص\n",
    "# min_df تتجاهل الكلمات التي تظهر في عدد قليل جدًا من الوثائق (مثلاً أقل من 5 وثائق)\n",
    "# ngram_range=(1,2) يمكن أن يلتقط كلمات مفردة وثنائيات الكلمات (مثل 'forest fire')\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, ngram_range=(1, 2))\n",
    "\n",
    "# تطبيق TF-IDF على عمود 'cleaned_text' في بيانات التدريب\n",
    "# .fit_transform يتعلم المفردات من بيانات التدريب ثم يحولها\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "\n",
    "# تطبيق نفس TF-IDF على عمود 'cleaned_text' في بيانات الاختبار\n",
    "# .transform فقط يحول النصوص بناءً على المفردات التي تعلمها من بيانات التدريب\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "print(\"تم تحويل النصوص إلى ميزات رقمية باستخدام TF-IDF بنجاح.\")\n",
    "print(f\"شكل مصفوفة ميزات التدريب (X_train_tfidf): {X_train_tfidf.shape}\")\n",
    "print(f\"شكل مصفوفة ميزات الاختبار (X_test_tfidf): {X_test_tfidf.shape}\")\n",
    "\n",
    "# يمكننا أيضًا عرض بعض أسماء الميزات (الكلمات)\n",
    "# print(\"\\nأول 20 ميزة (كلمة) من TF-IDF Vectorizer:\")\n",
    "# print(tfidf_vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "571f7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم دمج الميزات بنجاح.\n",
      "شكل مصفوفة ميزات التدريب المدمجة (X_train_combined): (7613, 3268)\n",
      "شكل مصفوفة ميزات الاختبار المدمجة (X_test_combined): (3263, 3268)\n"
     ]
    }
   ],
   "source": [
    "# الخلية 15: دمج ميزات TF-IDF مع الميزات الإضافية\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np # قد تحتاجها لتحويل الميزات الجديدة إلى مصفوفة NumPy\n",
    "\n",
    "# استخراج الميزات الإضافية من train_df و test_df\n",
    "# تأكد من أن أسماء الأعمدة تتطابق تماماً\n",
    "additional_features_train = train_df[['text_len', 'word_count', 'punct_count', 'upper_case_ratio', 'has_keyword', 'has_location']].values\n",
    "additional_features_test = test_df[['text_len', 'word_count', 'punct_count', 'upper_case_ratio', 'has_keyword', 'has_location']].values\n",
    "\n",
    "# دمج ميزات TF-IDF مع الميزات الإضافية\n",
    "# hstack تستخدم لدمج المصفوفات أفقياً (بإضافة أعمدة)\n",
    "# ملاحظة: X_train_tfidf و X_test_tfidf يجب أن تكونا موجودتين من الخلية 9\n",
    "X_train_combined = hstack([X_train_tfidf, additional_features_train])\n",
    "X_test_combined = hstack([X_test_tfidf, additional_features_test])\n",
    "\n",
    "print(\"تم دمج الميزات بنجاح.\")\n",
    "print(f\"شكل مصفوفة ميزات التدريب المدمجة (X_train_combined): {X_train_combined.shape}\")\n",
    "print(f\"شكل مصفوفة ميزات الاختبار المدمجة (X_test_combined): {X_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f92d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شكل بيانات التدريب المقسمة (X_train_split): (6090, 3262)\n",
      "شكل بيانات التحقق المقسمة (X_val_split): (1523, 3262)\n",
      "\n",
      "بدء تدريب نموذج Logistic Regression...\n",
      "تم تدريب النموذج بنجاح.\n",
      "\n",
      "أداء النموذج على مجموعة التحقق (Validation Set):\n",
      "F1-Score: 0.7632\n",
      "\n",
      "تقرير التصنيف (Classification Report):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       869\n",
      "           1       0.83      0.70      0.76       654\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.82      0.80      0.80      1523\n",
      "weighted avg       0.81      0.81      0.81      1523\n",
      "\n",
      "\n",
      "عمل تنبؤات على بيانات الاختبار (test_df) النهائية...\n",
      "تم إنشاء التنبؤات لبيانات الاختبار.\n"
     ]
    }
   ],
   "source": [
    "# الخلية 10: بناء وتدريب نموذج Logistic Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1. تقسيم بيانات التدريب إلى مجموعتي تدريب وتحقق\n",
    "# الهدف: X_train_tfidf (الميزات)، y_train (الهدف)\n",
    "y_train = train_df['target'] # عمود الهدف من بيانات التدريب الأصلية\n",
    "\n",
    "# نقسم بيانات التدريب إلى 80% للتدريب و 20% للتحقق\n",
    "# stratify=y_train يضمن أن تكون نسبة الفئات (0 و 1) متساوية في كلا المجموعتين\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_tfidf, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"شكل بيانات التدريب المقسمة (X_train_split): {X_train_split.shape}\")\n",
    "print(f\"شكل بيانات التحقق المقسمة (X_val_split): {X_val_split.shape}\")\n",
    "\n",
    "# 2. تهيئة وتدريب نموذج Logistic Regression\n",
    "print(\"\\nبدء تدريب نموذج Logistic Regression...\")\n",
    "model = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "model.fit(X_train_split, y_train_split)\n",
    "print(\"تم تدريب النموذج بنجاح.\")\n",
    "\n",
    "# 3. تقييم أداء النموذج على مجموعة التحقق\n",
    "y_val_pred = model.predict(X_val_split)\n",
    "f1_val = f1_score(y_val_split, y_val_pred)\n",
    "\n",
    "print(f\"\\nأداء النموذج على مجموعة التحقق (Validation Set):\")\n",
    "print(f\"F1-Score: {f1_val:.4f}\")\n",
    "print(\"\\nتقرير التصنيف (Classification Report):\")\n",
    "print(classification_report(y_val_split, y_val_pred))\n",
    "\n",
    "# 4. عمل تنبؤات على بيانات الاختبار النهائية\n",
    "print(\"\\nعمل تنبؤات على بيانات الاختبار (test_df) النهائية...\")\n",
    "test_predictions = model.predict(X_test_tfidf)\n",
    "print(\"تم إنشاء التنبؤات لبيانات الاختبار.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6657f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تم إنشاء ملف الإرسال 'submission.csv' بنجاح!\n",
      "\n",
      "أول 5 صفوف من ملف الإرسال:\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "# الخلية 11: إنشاء ملف الإرسال لـ Kaggle\n",
    "\n",
    "# إنشاء DataFrame جديد لملف الإرسال\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],  # نأخذ عمود 'id' من بيانات الاختبار الأصلية\n",
    "    'target': test_predictions # نستخدم التنبؤات التي حصلنا عليها من النموذج\n",
    "})\n",
    "\n",
    "# حفظ DataFrame كملف CSV\n",
    "# index=False لمنع pandas من كتابة فهرس الصفوف كعمود إضافي في الملف\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"تم إنشاء ملف الإرسال 'submission.csv' بنجاح!\")\n",
    "print(\"\\nأول 5 صفوف من ملف الإرسال:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac3085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
